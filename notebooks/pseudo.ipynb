{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from tsv.natvamp import DLSV\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import seaborn as sns\n",
    "\n",
    "fmnist_train = FashionMNIST(\n",
    "    \"FMNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    ")\n",
    "control_data = fmnist_train.data.view(-1, 1, 28, 28).float().numpy()\n",
    "control_data /= 255\n",
    "control_labels = fmnist_train.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ModularNVPW.load_from_checkpoint(\"/home/zain/code/two-stage/logs/nvpw/fmnist-pseudodiverge/checkpoints/epoch=99-step=5900.ckpt\")\n",
    "model = DLSV.load_from_checkpoint(\"/home/zain/code/two-stage/logs/dlsv/fmnist-stochastic/checkpoints/epoch=142-step=33605.ckpt\")\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pseudos_and_representatives(pseudos, representatives):\n",
    "    num_pseudos = pseudos.shape[0]\n",
    "    num_reps = representatives.shape[0]\n",
    "    fig, axs = plt.subplots(\n",
    "        num_pseudos, num_reps + 1, figsize=(8, 28)\n",
    "    )\n",
    "    for i, ax in enumerate(axs):\n",
    "        pseudo = pseudos[i, 0]\n",
    "        ax[0].imshow(pseudo)\n",
    "        ax[0].axis(\"off\")\n",
    "        for j in range(1, num_reps + 1):\n",
    "            rep = representatives[j - 1, i]\n",
    "            ax[j].imshow(rep)\n",
    "            ax[j].axis(\"off\")\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pseudos():\n",
    "    width = np.ceil(np.sqrt(model.num_pseudos))\n",
    "    width = int(width)\n",
    "    length = width\n",
    "    if model.num_pseudos % width == 0:\n",
    "        length = model.num_pseudos // width\n",
    "    fig, ax = plt.subplots(length, width, figsize=(14, 10))\n",
    "\n",
    "    for i, _ax in zip(range(model.num_pseudos), ax.flatten()):\n",
    "        pseudo = model.pseudos[i].cpu().view(28, 28).numpy()\n",
    "        _ax.imshow(pseudo)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from matplotlib.patches import Ellipse\n",
    "import tqdm\n",
    "\n",
    "def generate_embedding(train_dataloader, encoder, transform):\n",
    "    embeddings = []\n",
    "    targets = []\n",
    "    for batch in tqdm.tqdm(train_dataloader):\n",
    "        x, y = batch\n",
    "        x = x.view(-1, 1, 28, 28).float().cuda()\n",
    "        x /= 255\n",
    "        z = encoder(x)\n",
    "        z = transform(z)\n",
    "        embeddings.append(z.detach().cpu().numpy())\n",
    "        targets.append(y.detach().cpu().numpy())\n",
    "    embeddings = np.concatenate(embeddings)\n",
    "    targets = np.concatenate(targets)\n",
    "    return embeddings, targets\n",
    "\n",
    "def plot_embedding(embeddings, targets, model):\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "    if embeddings.shape[-1] > 2:\n",
    "        reducer = umap.UMAP(min_dist=0)\n",
    "\n",
    "        visual_embedding = reducer.fit_transform(embeddings)\n",
    "        if mu_p is not None:\n",
    "            mu_p = mu_p.detach().cpu().numpy()\n",
    "            embedded_pseudos = reducer.transform(mu_p)\n",
    "    else:\n",
    "        mu_p = None\n",
    "        logvar_p = None\n",
    "        if hasattr(model, \"pseudos\"):\n",
    "            mu_p, logvar_p, *_ = model.q_z(model.get_pseudos())\n",
    "        if logvar_p is not None:\n",
    "            logvar_p = logvar_p.detach().cpu().numpy()\n",
    "            std_p = np.exp(0.5 * logvar_p)\n",
    "        visual_embedding = embeddings\n",
    "        if mu_p is not None:\n",
    "            embedded_pseudos = mu_p.detach().cpu().numpy()\n",
    "            for embedded_pseudo, std in zip(embedded_pseudos, std_p):\n",
    "                print(f\"Making ellipse at {embedded_pseudo} with std {std}\")\n",
    "                axes.add_patch(\n",
    "                    Ellipse(\n",
    "                        xy=embedded_pseudo,\n",
    "                        width=3 * std[0],\n",
    "                        height=3 * std[1],\n",
    "                        edgecolor=\"r\",\n",
    "                        fc=\"grey\",\n",
    "                        lw=2,\n",
    "                    )\n",
    "                )\n",
    "    assert targets is not None\n",
    "    axes.scatter(\n",
    "        visual_embedding[:, 0],\n",
    "        visual_embedding[:, 1],\n",
    "        c=targets,\n",
    "        s=0.75,\n",
    "        cmap=\"tab10\",\n",
    "    )\n",
    "    if mu_p is not None:\n",
    "        axes.scatter(\n",
    "            embedded_pseudos[:, 0],\n",
    "            embedded_pseudos[:, 1],\n",
    "            c=\"black\",\n",
    "            s=50,\n",
    "            marker=\"x\",\n",
    "        )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pseudos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsv.data import FMNISTDataModule\n",
    "\n",
    "data_module = FMNISTDataModule(batch_size=256, num_workers=4, persistent_workers=False)\n",
    "data_module.setup('fit')\n",
    "data_loader = data_module.train_dataloader()\n",
    "embeddings, targets = generate_embedding(data_loader, lambda x: model.q_z(x)[0], lambda x: x)\n",
    "plot_embedding(embeddings, targets, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.project_pseudos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_embedding(data_loader, lambda x: model.q_z(x)[0], lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pseudos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_divergences(model, indices):\n",
    "    divergences = np.full((len(indices), len(indices)), np.inf)\n",
    "    for idx, jdx in product(indices, indices):\n",
    "        if idx == jdx:\n",
    "            continue\n",
    "        x, y = model.get_pseudos()[[idx, jdx]]\n",
    "        x = x.unsqueeze(0)\n",
    "        y = y.unsqueeze(0)\n",
    "        kl_div = model.general_kl(*model.q_z(x)[:2], *model.q_z(y)[:2])\n",
    "        divergences[idx, jdx] = kl_div.item()\n",
    "    return divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_twins(divergences_triplet):\n",
    "    twins = []\n",
    "    for idx, jdx in divergences_triplet:\n",
    "        if (jdx, idx) in twins or (idx, jdx) in twins:\n",
    "            continue\n",
    "        if (jdx, idx) in divergences_triplet:\n",
    "            if jdx < idx:\n",
    "                idx, jdx = jdx, idx\n",
    "            twins.append((idx, jdx))\n",
    "    return twins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_merges(model):\n",
    "    divergences = calculate_divergences(model, range(model.num_pseudos))\n",
    "    sorted_divergences = np.argmin(divergences, axis=1)\n",
    "    twin_idxs = np.argsort(divergences[np.arange(divergences.shape[0]), sorted_divergences])    \n",
    "    divergences_triplet = {}\n",
    "    for idx in twin_idxs:\n",
    "        divergences_triplet[(idx, sorted_divergences[idx])] = divergences[idx, sorted_divergences[idx]]\n",
    "    twins = _find_twins(divergences_triplet)\n",
    "    sorted_twins = sorted(twins, reverse=True, key=lambda x: x[1])\n",
    "    return sorted_twins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = propose_merges(model)\n",
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "data_loader = DataLoader(\n",
    "            fmnist_train.train_data,\n",
    "            batch_size=256,\n",
    "            num_workers=4,\n",
    "            shuffle=False,\n",
    "            persistent_workers=False,\n",
    "            pin_memory=True,\n",
    "            prefetch_factor=5,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsv.natvamp import log_normal_diag\n",
    "\n",
    "pseudos = model.get_pseudos()\n",
    "mu_p, logvar_p, *_ = model.q_z(pseudos)\n",
    "max_likelihood_idx = []\n",
    "likelihoods = []\n",
    "for batch in tqdm.tqdm(data_loader):\n",
    "    x = batch.float().cuda().view(-1, 1, 28, 28)\n",
    "    x /= 255\n",
    "    mu = model.q_z(x)[0]\n",
    "    likelihoods.append(log_normal_diag(mu.unsqueeze(1), mu_p.unsqueeze(0), logvar_p.unsqueeze(0), reduction=\"sum\", dim=-1).cpu())\n",
    "likelihoods = torch.cat(likelihoods, 0)\n",
    "max_likelihood_idx = likelihoods.argmax(1).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pseudos, counts = np.unique(max_likelihood_idx, return_counts=True)\n",
    "ordered_pseudo_idxs = unique_pseudos[np.argsort(counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_pseudo_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[ordered_pseudo_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divergences = calculate_divergences(model, np.arange(model.num_pseudos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_idx = divergences.argmin(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "must_murge_idxs = ordered_pseudo_idxs[counts[ordered_pseudo_idxs]<100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_target_idxs = closest_idx[must_murge_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(idx, closest_idx[idx]) for idx in must_murge_idxs])\n",
    "print([(11, 19), (9, 13), (2, 10), (5, 7), (0, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.merge_pseudos(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pseudos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"/home/zain/code/two-stage/logs/nvpw/fmnist-pseudodiverge/merge_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "data_loader = DataLoader(\n",
    "            fmnist_train.train_data,\n",
    "            batch_size=256,\n",
    "            num_workers=4,\n",
    "            shuffle=False,\n",
    "            persistent_workers=False,\n",
    "            pin_memory=True,\n",
    "            prefetch_factor=5,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsv.natvamp import log_normal_diag\n",
    "\n",
    "pseudos = model.get_pseudos()\n",
    "mu_p, logvar_p, *_ = model.q_z(pseudos)\n",
    "max_likelihood_idx = []\n",
    "likelihoods = []\n",
    "for batch in tqdm.tqdm(data_loader):\n",
    "    x = batch.float().cuda().view(-1, 1, 28, 28)\n",
    "    x /= 255\n",
    "    mu = model.q_z(x)[0]\n",
    "    likelihoods.append(log_normal_diag(mu.unsqueeze(1), mu_p.unsqueeze(0), logvar_p.unsqueeze(0), reduction=\"sum\", dim=-1).cpu())\n",
    "likelihoods = torch.cat(likelihoods, 0)\n",
    "max_likelihood_idx = likelihoods.argmax(1).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_representatives(pseudo_idx, num_representatives):\n",
    "    base_idx = np.arange(likelihoods.shape[0])\n",
    "    paragon_mask = max_likelihood_idx == pseudo_idx\n",
    "    print(np.sum(paragon_mask))\n",
    "    base_idx = base_idx[paragon_mask]\n",
    "    paragon_likelihoods = likelihoods[paragon_mask, pseudo_idx]\n",
    "    vals, indices = torch.sort(paragon_likelihoods.cpu(), descending=True)\n",
    "    if len(indices) >= num_representatives:\n",
    "        return base_idx[indices][:num_representatives]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "plt.imshow(control_data[find_representatives(0, 3)[idx]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pseudos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _yield_representatives():\n",
    "    for i in range(model.num_pseudos):\n",
    "        representatives = find_representatives(i, 3)\n",
    "        if representatives is not None:\n",
    "            yield representatives\n",
    "\n",
    "representatives = [torch.tensor(control_data[rep]) for rep in _yield_representatives()]\n",
    "print([rep.shape for rep in representatives])\n",
    "representatives = torch.cat(representatives, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representatives.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pseudos_and_representatives(pseudos.cpu(), representatives.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsvb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
